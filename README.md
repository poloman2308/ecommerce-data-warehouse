# ğŸ¬ E-commerce Data Warehouse

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0)
[![PostgreSQL License](https://img.shields.io/badge/License-PostgreSQL-blue.svg)](https://opensource.org/licenses/PostgreSQL)
[![BSD 3-Clause](https://img.shields.io/badge/License-BSD_3--Clause-orange.svg)](https://opensource.org/licenses/BSD-3-Clause)
[![LGPL](https://img.shields.io/badge/License-LGPL-lightgrey.svg)](https://www.gnu.org/licenses/lgpl-3.0.html)
[![CI](https://github.com/poloman2308/ecommerce-data-warehouse/actions/workflows/dbt.yml/badge.svg)](https://github.com/poloman2308/ecommerce-data-warehouse/actions)

A modern data warehouse pipeline for e-commerce analytics â€” built with **dbt**, **Apache Airflow**, **PostgreSQL**, and **Docker**. This project follows ELT best practices, automates freshness checks, generates documentation, and orchestrates tasks via a production-ready Airflow DAG.

---

## ğŸ§± Architecture

```mermaid
flowchart LR
  subgraph Source_Data
    A1[raw_orders.csv]
    A2[raw_customers.csv]
    A3[raw_products.csv]
    A4[raw_returns.csv]
  end

  subgraph Bronze_Layer
    B1[init_db.sql â†’ PostgreSQL]
    A1 --> B1
    A2 --> B1
    A3 --> B1
    A4 --> B1
    B1 --> C1[raw schema]
  end

  subgraph Orchestration
    D1[Airflow DAG: dbt_dag.py]
    C1 --> D1
  end

  subgraph dbt_Transformation
    D1 --> E1[dbt build: staging models]
    E1 --> E2[dbt build: marts layer]
    D1 --> F1[dbt source freshness]
    D1 --> F2[dbt docs generate and serve]
  end
```

---

## ğŸš€ Features

* Airflow DAG: Automates CSV load, dbt model builds, source freshness checks, and doc generation.

* dbt Models: Modular staging and marts layers using best practices.

* Data Quality Tests: Built-in tests for not_null, unique, and freshness.

* Dockerized: Fully containerized dev environment.

* Documentation Site: Self-hosted dbt docs via Airflow task.

---

## ğŸ“‚ Project Structure

```bash
ecommerce-data-warehouse/
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ dbt_dag.py              # Airflow DAG definition
â”‚
â”œâ”€â”€ dbt_project/
â”‚   â””â”€â”€ ecommerce_dbt/
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ staging/            # Staging models (stg_*)
â”‚       â”‚   â””â”€â”€ marts/              # Fact/dimension models
â”‚       â”œâ”€â”€ seeds/                  # Optional seed data
â”‚       â”œâ”€â”€ dbt_project.yml         # DBT project config
â”‚       â””â”€â”€ profiles.yml            # DBT profile
â”‚
â”œâ”€â”€ generate_dim_date.py           # Script to generate date dimension
â”œâ”€â”€ init_db.sql                    # Creates raw schema and loads CSVs
â”œâ”€â”€ docker-compose.yml             # Spin up Postgres, Airflow, and dbt
â””â”€â”€ README.md
```

---

## ğŸ§ª How to Run Locally

### 1. Clone the repo

```bash
git clone https://github.com/poloman2308/ecommerce-data-warehouse.git
cd ecommerce-data-warehouse
```

### 2. Launch containers

```bash
docker compose up --build
```

### 3. Access Services

* Airflow UI: http://localhost:8080 (login: airflow / airflow)

* DBT Docs: http://localhost:8081

* Postgres: localhost:5432 (user: airflow, db: airflow)

### 4. Trigger DAG

* Navigate to Airflow UI â†’ Turn on dbt_dag_ecommerce

---

## âš™ï¸ Airflow DAG Overview

| Task ID                | Description                          |
| ---------------------- | ------------------------------------ |
| `init_db`              | Loads CSVs into PostgreSQL (`raw.*`) |
| `dbt_source_freshness` | Runs `dbt source freshness` checks   |
| `dbt_build`            | Runs `dbt build` (models + tests)    |
| `dbt_docs_generate`    | Generates static HTML docs           |
| `dbt_docs_serve`       | Serves docs at port 8081             |

---

## ğŸ“Š Example Models

Fact Table: fct_orders
* Joins orders, customers, products, returns, and date_dim

* Enriched with order status, return flags, and derived date fields

Dimension Table: dim_customers
* Extracted from raw_customers

* Includes signup date, email domain, and lifecycle segmentation

---

## ğŸ” Environment Variables

Create a .env (optional if customizing):

```env
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
```

---

## ğŸ§  Future Enhancements

* Add real-time ingestion via Kafka

* CI/CD with GitHub Actions for dbt build/test

* Data visualizations with Metabase or Superset

* Add pytest or Great Expectations for validation

---

## ğŸ“š Table of Contents

- [Architecture](#-architecture)
- [Features](#-features)
- [Project Structure](#-project-structure)
- [How to Run Locally](#-how-to-run-locally)
- [Airflow DAG Overview](#ï¸-airflow-dag-overview)
- [Example Models](#-example-models)
- [Environment Variables](#-environment-variables)
- [Future Enhancements](#-future-enhancements)
- [Author](#-author)

---

## ğŸ™‹â€â™‚ï¸ Author

**Derek Acevedo**  
ğŸ“ [GitHub](https://github.com/poloman2308)  
ğŸ“„ [LinkedIn](https://www.linkedin.com/in/derekacevedo86)


